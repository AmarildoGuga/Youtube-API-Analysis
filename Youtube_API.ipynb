{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import JSON\n",
    "from dateutil import parser\n",
    "import isodate\n",
    "\n",
    "# Environment FIle\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Data visualisation libraries\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "#Data Visual Pakcages\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#Dissable SSL Veriication - Recommend you delete this \n",
    "import ssl\n",
    "import nltk\n",
    "\n",
    "# Disable SSL verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Re-enable SSL verification\n",
    "ssl._create_default_https_context = ssl.create_default_context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering using Youtube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Youtube API Key stored in an env file\n",
    "api_key = os.getenv('api_key')\n",
    "\n",
    "# Pewdiepie youtube Channel ID = UC-lHJZR3Gqxm24_Vd_AJ5Yw\n",
    "# Ali-Abdaal youtube Channel ID = UCoOae5nYA7VqaXzerajD0lg\n",
    "# David Dobrik youtube Channel ID = UCmh5gdwCx6lN7gEC20leNVA\n",
    "channel_ids = ['UCmh5gdwCx6lN7gEC20leNVA',\n",
    "               #more chanels can be placed here if you want\n",
    "               ]\n",
    "\n",
    "#Documentation file from youtube V3 data pack to get a users youtube data as a list\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "# Get credentials and create an API client\n",
    "youtube = build(\n",
    "    api_service_name, api_version, developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function that will get all the chanels stats and store it into a df\n",
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=','.join(channel_ids) #joins the channel ids if you have more than 1\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    #loop through items\n",
    "    for item in response['items']:\n",
    "        data = {'ChannelName': item['snippet']['title'],\n",
    "                'subscribers': item['statistics']['subscriberCount'],\n",
    "                'views': item['statistics']['viewCount'],\n",
    "                'totalVideos': item['statistics']['videoCount'],\n",
    "                'playlistID': item['contentDetails']['relatedPlaylists']['uploads']\n",
    "                \n",
    "        }\n",
    "        \n",
    "        all_data.append(data)\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def get_video_ids(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    Get list of video IDs of all videos in the given playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    playlist_id: playlist ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    List of video IDs of all videos in the playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    video_ids = []\n",
    "\n",
    "    request = youtube.playlistItems().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        playlistId=playlist_id,\n",
    "        maxResults = 50\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    for item in response['items']:\n",
    "        video_ids.append(item['contentDetails']['videoId'])\n",
    "    \n",
    "    #Youtube API limits you to 50 requests per page - work around \n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    while next_page_token is not None:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            playlistId=playlist_id,\n",
    "            maxResults = 50,\n",
    "            pageToken = next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "            \n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "    return video_ids\n",
    "\n",
    "\n",
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get video statistics of all videos with given IDs\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with statistics of videos, i.e.:\n",
    "        'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n",
    "        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n",
    "        'duration', 'definition', 'caption'\n",
    "    \"\"\"\n",
    "    \n",
    "    all_video_info = []\n",
    "\n",
    "    for i in range(0, len(video_ids), 50):  ##Takes all the videos that are present in the playlist\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id = ','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n",
    "                             'contentDetails': ['duration', 'definition', 'caption']             \n",
    "                            }\n",
    "\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "            \n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    #If some stats are missing from the video then we can have an error management = ignore the missing part\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "                        \n",
    "            \n",
    "            all_video_info.append(video_info)\n",
    "        \n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "\n",
    "def get_comments_in_videos(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with video IDs and associated top level comment in text.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "        try:   \n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "        \n",
    "            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n",
    "            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n",
    "\n",
    "            all_comments.append(comments_in_video_info)\n",
    "            \n",
    "        except: \n",
    "            # When error occurs - most likely because comments are disabled on a video\n",
    "            print('Could not get comments for video ' + video_id)\n",
    "        \n",
    "    return pd.DataFrame(all_comments)     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the functions to get the channel stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_stats = get_channel_stats(youtube, channel_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running the 'channel_stats' function to view what it has pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a playlist from the overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the channel_id is used as the playlist_id for all the youtubers uploaded videos\n",
    "#Choose a playlist you would like to analyse\n",
    "playlist_id = 'UUoOae5nYA7VqaXzerajD0lg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data fram with youtube video stats and the comments from all videos in the playlist\n",
    "video_ids = get_video_ids(youtube, playlist_id)\n",
    "video_df = get_video_details(youtube, video_ids)\n",
    "\n",
    "#The comments usually take the longest time so if you want to save time # this out :D\n",
    "comments_df = get_comments_in_videos(youtube, video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write video data to CSV file for future references\n",
    "video_df.to_csv('video_data.csv')\n",
    "comments_df.to_csv('comments_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting some object columns into numeric\n",
    "numeric_cols = ['viewCount', 'likeCount', 'favouriteCount', 'commentCount']\n",
    "video_df[numeric_cols] = video_df[numeric_cols].apply(pd.to_numeric, errors = 'coerce', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Published day in the week\n",
    "video_df['publishedAt'] = video_df['publishedAt'].apply(lambda x: parser.parse(x))\n",
    "video_df['publishedDayName'] = video_df['publishedAt'].apply(lambda x: x.strftime(\"%A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add tag count\n",
    "video_df['tagCount'] = video_df['tags'].apply(lambda x: 0 if x is None else len(x))\n",
    "#there are some nulls so nulls will be replaced by 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments and likes per 1000 view ratio\n",
    "video_df['likeRatio'] = video_df['likeCount']/ video_df['viewCount'] * 1000\n",
    "video_df['commentRatio'] = video_df['commentCount']/ video_df['viewCount'] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title character length\n",
    "video_df['titleLength'] = video_df['title'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the duration of a video from a string of numbers and letters to a seconds\n",
    "import isodate\n",
    "video_df['durationSecs'] = video_df['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())\n",
    "##video_df['durationSecs'] = video_df['durationSecs'].astype('timedelta64[s]') ### this is not working idk why --- its supposed to convert the 0 days 00:00:00 to 000.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the output to see if the duratoin changed to seconds\n",
    "video_df[['durationSecs', 'duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Performing Videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x = 'title', y = 'viewCount', data = video_df.sort_values('viewCount', ascending=False) [0:9])\n",
    "plot = ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000)+ 'K'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Performing Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x = 'title', y = 'viewCount', data = video_df.sort_values('viewCount', ascending=True) [0:9])\n",
    "plot = ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000)+ 'K'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Distribution Per Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=video_df['channelTitle'], y=video_df['viewCount'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views vs Likes and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "sns.scatterplot(data = video_df, x = 'commentCount', y = 'viewCount', ax = ax[0])\n",
    "sns.scatterplot(data = video_df, x = 'likeCount', y = 'viewCount', ax = ax[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = video_df, x = 'durationSecs', bins = 30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title length vs views\n",
    "#### Checking to see if the title length as an indication for views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.scatterplot(x = 'titleLength', y = 'viewCount', data = video_df.sort_values('viewCount', ascending=True))\n",
    "# Increase spacing between y-axis ticks\n",
    "# Set y-axis tick locator to multiples of 100\n",
    "plt.gca().yaxis.set_major_locator(ticker.MultipleLocator(base=100))\n",
    "# Invert the y-axis direction\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud for video Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "video_df['title_no_stopwords'] = video_df['title'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n",
    "\n",
    "all_words = list([a for b in video_df['title_no_stopwords'].tolist() for a in b])\n",
    "all_words_str = ' '.join(all_words) \n",
    "\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\");\n",
    "\n",
    "wordcloud = WordCloud(width = 2000, height = 1000, random_state=1, background_color='black', \n",
    "                      colormap='viridis', collocations=False).generate(all_words_str)\n",
    "plot_cloud(wordcloud)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = pd.DataFrame(video_df['publishedDayName'].value_counts())\n",
    "weekdays = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_df = day_df.reindex(weekdays)\n",
    "ax = day_df.plot.bar(rot=0)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Count')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud for Video Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "comments_df['comments_no_stopwords'] = comments_df['comments'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n",
    "\n",
    "all_words = list([a for b in comments_df['comments_no_stopwords'].tolist() for a in b])\n",
    "all_words_str = ' '.join(all_words) \n",
    "\n",
    "wordcloud = WordCloud(width = 2000, height = 1000, random_state=1, background_color='black', \n",
    "                      colormap='viridis', collocations=False).generate(all_words_str)\n",
    "plot_cloud(wordcloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
